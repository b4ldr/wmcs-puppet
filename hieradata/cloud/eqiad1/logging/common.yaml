kafka_clusters:
  logging-beta:
    zookeeper_cluster_name: main-deployment-prep
    brokers:
      deployment-kafka-logging01.deployment-prep.eqiad1.wikimedia.cloud:
        id: 1001
        rack: B

zookeeper_clusters:
  main-deployment-prep:
    hosts:
      deployment-zookeeper02.eqiad.wmflabs: '2'

prometheus_nodes:
  - logging-alert-01.logging.eqiad1.wikimedia.cloud

# known public
profile::logstash::collector::input_kafka_ssl_truststore_passwords:
  logging-beta: qwerty
profile::logstash::collector::input_kafka_consumer_group_id: 'logging-logstash'

profile::java::java_packages:
  - version: '11'
    variant: 'jdk'

profile::opensearch::dashboards::httpd_proxy::auth_file: '/etc/logstash/htpasswd'
profile::opensearch::dashboards::httpd_proxy::auth_type: 'local'
profile::opensearch::dashboards::httpd_proxy::auth_realm: 'local'
profile::opensearch::dashboards::httpd_proxy::vhost: 'beta-logs.wmcloud.org'
profile::opensearch::dashboards::httpd_proxy::serveradmin: 'root@logging-logstash-01.logging.eqiad1.wikimedia.cloud'

# phatality will need ported before it can be enabled
profile::opensearch::dashboards::enable_phatality: false

profile::opensearch::rack: ''
profile::opensearch::row: ''
profile::opensearch::instances: {}
profile::opensearch::base_data_dir: '/srv/opensearch'
profile::opensearch::dc_settings:
  cluster_name: 'logging-beta'
  short_cluster_name: beta
  cluster_hosts:
    - logging-opensearch-ssd-01.logging.eqiad1.wikimedia.cloud
    - logging-opensearch-ssd-02.logging.eqiad1.wikimedia.cloud
    - logging-opensearch-ssd-03.logging.eqiad1.wikimedia.cloud
    - logging-opensearch-hdd-01.logging.eqiad1.wikimedia.cloud
    - logging-logstash-01.logging.eqiad1.wikimedia.cloud
  unicast_hosts:
    - logging-opensearch-ssd-01.logging.eqiad1.wikimedia.cloud
    - logging-opensearch-ssd-02.logging.eqiad1.wikimedia.cloud
    - logging-opensearch-ssd-03.logging.eqiad1.wikimedia.cloud
    - logging-opensearch-hdd-01.logging.eqiad1.wikimedia.cloud

profile::opensearch::common_settings:
  auto_create_index: true
  awareness_attributes: ''
  minimum_master_nodes: 2
  recover_after_nodes: 2
  recover_after_time: '1m'
  # Dont encourage some sort of accidental feedback loop
  send_logs_to_logstash: false
  http_port: 9200
  transport_tcp_port: 9300
  curator_uses_unicast_hosts: false
  filter_cache_size: '10%'
  holds_data: false
  compatibility_mode: true

profile::opensearch::logstash::jobs_host: 'logging-logstash-01.logging.eqiad1.wikimedia.cloud'
profile::opensearch::logstash::curator_actions:
  '01':
    description: 'logstash: delete older than 14 days'
    action: delete_indices
    options:
      ignore_empty_list: true
    filters:
      - filtertype: pattern
        kind: prefix
        value: 'logstash-'
      - filtertype: age
        source: name
        direction: older
        timestring: '%Y.%m.%d'
        unit: days
        unit_count: 14

  '02':
    description: 'dlq: delete older than 2 days'
    action: delete_indices
    options:
      ignore_empty_list: true
    filters:
      - filtertype: pattern
        kind: prefix
        value: 'dlq-'
      - filtertype: age
        source: name
        direction: older
        timestring: '%Y.%m.%d'
        unit: days
        unit_count: 2

  '03':
    description: 'ecs-1.7.0 default: delete older than 2 weeks'
    action: delete_indices
    options:
      ignore_empty_list: true
    filters:
      - filtertype: pattern
        kind: regex
        value: '^ecs-1.7.0-\d\d?-default-.*$'
      - filtertype: age
        source: name
        direction: older
        timestring: '%Y.%W'
        unit: weeks
        unit_count: 2

  '04':
    description: 'ecs-1.7.0 test: delete older than 2 weeks'
    action: delete_indices
    options:
      ignore_empty_list: true
    filters:
      - filtertype: pattern
        kind: regex
        value: '^ecs-1.7.0-\d\d?-test-.*$'
      - filtertype: age
        source: name
        direction: older
        timestring: '%Y.%W'
        unit: weeks
        unit_count: 2

  '05':
    description: 'ecs-1.7.0 alerts: delete older than 2 weeks'
    action: delete_indices
    options:
      ignore_empty_list: true
    filters:
      - filtertype: pattern
        kind: regex
        value: '^ecs-1.7.0-\d\d?-alerts-.*$'
      - filtertype: age
        source: name
        direction: older
        timestring: '%Y.%W'
        unit: weeks
        unit_count: 2

  '06':
    description: 'w3creportingapi-1.0.0: delete older than 2 weeks'
    action: delete_indices
    options:
      ignore_empty_list: true
    filters:
      - filtertype: pattern
        kind: regex
        value: '^w3creportingapi-1.0.0-[2-9]-.*$'
      - filtertype: age
        source: name
        direction: older
        timestring: '%Y.%W'
        unit: weeks
        unit_count: 2

  '20':
    description: 'all: after 1 days set number of replicas to 0'
    action: replicas
    options:
      count: 0
      ignore_empty_list: true
    filters:
      - filtertype: pattern
        kind: prefix
        exclude: true  # exclude special indexes
        value: '^\..*'
      - filtertype: age
        source: creation_date
        direction: older
        unit: days
        unit_count: 1

  # For any indices prefixed by "logstash-", and have "index.routing.allocation.require.disktype":"ssd" set,
  # and are older than 7 days, set "index.routing.allocation.require.disktype":"hdd".
  '40':
    description: 'logstash: allocate older shards on hdd nodes'
    action: allocation
    options:
      key: disktype
      value: hdd
      allocation_type: require
      ignore_empty_list: true
    filters:
      - filtertype: pattern
        kind: prefix
        value: 'logstash-'
      - filtertype: allocated
        key: disktype
        value: ssd
        allocation_type: require
        exclude: false
      - filtertype: age
        source: creation_date
        direction: older
        unit: days
        unit_count: 7

  # forceMerge "logstash-" prefixed indices older than 2 days (based on index creation_date) to 1 segments per shard.
  # Delay 120 seconds between each forceMerge operation to allow the cluster to quiesce.
  # This action will ignore indices already forceMerged to the same or fewer number of segments per shard,
  # so the 'forcemerged' filter is unneeded.
  '90':
    description: 'all: forcemerge indexes older than 2 days'
    action: forcemerge
    options:
      max_num_segments: 1
      delay: 120
      continue_if_exception: false
    filters:
      - filtertype: pattern
        kind: regex
        exclude: true  # exclude special indexes
        value: '^\..*'
      - filtertype: age
        source: creation_date
        direction: older
        unit: days
        unit_count: 2
