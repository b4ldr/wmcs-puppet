nagios_group: analytics_eqiad
cluster: analytics
profile::admin::groups:
  - analytics-admins

profile::hadoop::common::hadoop_cluster_name: 'analytics-hadoop'

profile::hive::client::hive_service_name: 'analytics-hive'

profile::oozie::client::oozie_service: 'analytics-oozie'

# Set the hive-site.xml file with group ownership 'analytics' so systemd timers
# can read the file.
profile::hive::client::config_files_group_ownership: 'analytics'

profile::analytics::cluster::hdfs_mount::monitoring_user: 'analytics'

profile::analytics::refinery::job::project_namespace_map::http_proxy: 'http://webproxy.eqiad.wmnet:8080'

profile::analytics::refinery::job::data_purge::public_druid_host: 'druid1004.eqiad.wmnet'

profile::statistics::base::enable_stat_host_addons: false
profile::statistics::base::mysql_credentials_group: 'analytics'

profile::debdeploy::client::exclude_mounts:
  - /mnt/hdfs

profile::kerberos::keytabs::keytabs_metadata:
  - role: 'analytics'
    owner: 'analytics'
    group: 'analytics'
    filename: 'analytics.keytab'
  - role: 'hadoop'
    owner: 'hdfs'
    group: 'hdfs'
    filename: 'hdfs.keytab'
    parent_dir_grp: 'hadoop'

# Context https://phabricator.wikimedia.org/T278353#6976509
profile::kerberos::client::dns_canonicalize_hostname: false

profile::java::java_packages:
  - version: '8'
    variant: 'jdk'
profile::java::extra_args: 'JAVA_TOOL_OPTIONS="-Dfile.encoding=UTF-8"'

profile::analytics::refinery::ensure_hdfs_dirs: true

profile::analytics::refinery::job::hdfs_cleaner::ensure_timer: present


# Set up airflow instances.
profile::airflow::instances:
  # airflow@analytics instance.
  analytics:
    # Since we set security: kerberos a keytab must be deployed for the service_user.
    service_user: analytics
    service_group: analytics
    monitoring_enabled: false
    airflow_config:
      core:
        dags_folder: /srv/deployment/analytics/refinery/airflow/dags
        security: kerberos
        executor: LocalExecutor
        # This can be an ERB template that will be rendered in airflow::instance.
        # db_user and db_password params should be set in puppet private
        # in profile::airflow::instances_secrets.
        sql_alchemy_conn: mysql://<%= @db_user %>:<%= @db_password %>@an-coord1001.eqiad.wmnet/airflow_analytics?ssl_ca=/etc/ssl/certs/Puppet_Internal_CA.pem
    connections:
      fs_local:
        conn_type: fs
        description: Local filesystem on the Airflow Scheduler node
      # TODO: add more as needed

profile::contacts::role_contacts: ['Analytics SREs']
