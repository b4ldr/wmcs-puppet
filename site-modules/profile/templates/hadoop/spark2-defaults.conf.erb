# NOTE: This file is managed by Puppet.

# Default system properties included when running spark-submit.
# This is useful for setting default environmental settings.

# Example:
# spark.master                     spark://master:7077
# spark.eventLog.enabled           true
# spark.eventLog.dir               hdfs://namenode:8021/directory
# spark.serializer                 org.apache.spark.serializer.KryoSerializer
# spark.driver.memory              5g
# spark.executor.extraJavaOptions  -XX:+PrintGCDetails -Dkey=value -Dnumbers="one two three"

# Dynamic allocation allows Spark to dynamically scale the cluster resources
# allocated for an application based on the workload. Only available in YARN mode.
# More info: https://spark.apache.org/docs/2.1.2/configuration.html#dynamic-allocation
spark.dynamicAllocation.enabled                     true
spark.shuffle.service.enabled                       true
spark.dynamicAllocation.executorIdleTimeout         60s
spark.dynamicAllocation.cachedExecutorIdleTimeout   3600s
spark.shuffle.io.maxRetries                         10
spark.shuffle.io.retryWait                          10s
<% if @executor_env_ld_lib_path -%>
spark.executorEnv.LD_LIBRARY_PATH                   <%= @executor_env_ld_lib_path %>
<% end -%>
<% if @hive_enabled -%>
spark.sql.catalogImplementation                     hive
<% end -%>
<% if @driver_port -%>
spark.driver.port                                   <%= @driver_port %>
<% end -%>
<% if @port_max_retries -%>
spark.port.maxRetries                               <%= @port_max_retries %>
<% end -%>
<% if @ui_port -%>
spark.ui.port                                       <%= @ui_port %>
<% end -%>
<% if @driver_blockmanager_port -%>
spark.driver.blockManager.port                      <%= @driver_blockmanager_port %>
<% end -%>
spark.yarn.archive                                  hdfs:///user/spark/share/lib/spark-<%= @spark_version %>-assembly.zip

# JVMs should use system proxy settings.
# The system proxy settings are configured via the env vars http_proxy, https_proxy, and no_proxy.
# NOTE: When we upgrade to Spark 3, we should use 'defaultJavaOptions', rather than 'extraJavaOptions'.
spark.driver.extraJavaOptions                     -Djava.net.useSystemProxies=True
spark.executor.extraJavaOptions                   -Djava.net.useSystemProxies=True

<% @extra_settings.sort.each do |key, value| -%>
<%= key %>   <%= value %>
<% end -%>

<% if @encryption_enabled -%>
spark.authenticate                                  true
# Spark IO encryption settings are not enabled (but listed anyway)
# since in some use cases (like Refine) they caused exceptions like
# 'java.io.IOException: Stream is corrupted' when shuffle files were
# compressed with lz4.
# spark.io.encryption.enabled                         true
# spark.io.encryption.keySizeBits                     256
# spark.io.encryption.keygen.algorithm                HmacSHA256
spark.network.crypto.enabled                        true
spark.network.crypto.keyFactoryAlgorithm            PBKDF2WithHmacSHA256
spark.network.crypto.keyLength                      256
spark.network.crypto.saslFallback                   false
<% end -%>

# Ensure that Python requests lib always use system CA certificates.
spark.yarn.appMasterEnv.REQUESTS_CA_BUNDLE          /etc/ssl/certs/ca-certificates.crt
spark.executorEnv.REQUESTS_CA_BUNDLE                /etc/ssl/certs/ca-certificates.crt

<% if @spark_version <= '2.4.4' -%>
# ARROW_PRE_0_15_IPC_FORMAT needs to be set in order for pyarrow >= 0.15.0 to work with Spark <= 2.4.4.
# When we upgrade to Spark > 2.4.4 we should remove these.
# https://spark.apache.org/docs/3.0.0-preview/sql-pyspark-pandas-with-arrow.html#compatibiliy-setting-for-pyarrow--0150-and-spark-23x-24x
# https://issues.apache.org/jira/browse/SPARK-29367
spark.yarn.appMasterEnv.ARROW_PRE_0_15_IPC_FORMAT   1
spark.executorEnv.ARROW_PRE_0_15_IPC_FORMAT         1
<% end -%>
