#!/usr/bin/make -f
#
# From https://raw.githubusercontent.com/ooni/sysadmin/master/scripts/metadb_s3_tarx
#
# This script fetches latest archived metadb PGDATA for PostgreSQL replica
# instance using all available CPUs for decompression.
#
# Usage: ./metadb_s3_tarx DEST=/mountpoint/pgdir fetch
#
# Don't forget to set `hot_standby = 'on'` if you plan to use the produced
# `recovery.conf` file.
#
# The restore process can be CPU-bounded or network-bounded depending on
# available bandwidth.  `xz` decompressor at a "modern" CPU utilizes ~80 Mbit/s
# of network bandwidth available for fetching from S3.  Parallel decompression
# may speedup recovery but it also comes with a cost of greater file
# fragmentation as `tar` does not use `fallocate`.
#
# The script to create the archived of PGDATA is `metadb_s3_tarz`.
#

PGVER := 9.6

NCPU := $(shell grep -c ^processor /proc/cpuinfo)
MAKEFLAGS += --jobs $(NCPU) --keep-going

THIS := $(word $(words $(MAKEFILE_LIST)),$(MAKEFILE_LIST))

ifndef DEST
    $(error Define DEST as a directory for the actual files, e.g. `make -f $(THIS) DEST=.`)
endif

KERN := $(shell uname -s)
MACH := $(shell uname -m)
BASE := https://ooni-data.s3.amazonaws.com/metadb/$(KERN)-$(MACH)-$(PGVER)/base
WAL := https://ooni-data.s3.amazonaws.com/metadb/$(KERN)-$(MACH)-$(PGVER)/wal
BOOT_ID := $(shell cat /proc/sys/kernel/random/boot_id)
# FLAGS is a subdir in $PWD to track success of targets.
ifndef FLAGS
    FLAGS := metadb_s3_tarx.flags.$(BOOT_ID)
endif

# -include fetches it with dependency, $(file <) is GMake 4.2, Ubuntu 16.04 still has 4.1
LATEST = $(shell cat $(FLAGS)/latest)

.PHONY : help fetch
help : # default target
	@echo 'Usage:'
	@echo '  ./$(THIS) DEST=$(DEST) help     show this message'
	@echo '  ./$(THIS) DEST=$(DEST) fetch    fetch PostgreSQL dump from `$(BASE)/$(LATEST)` to `$(DEST)`'

# every command on a separate line to simulate `-o pipefail`
$(FLAGS)/.keep :
	mkdir -p $(FLAGS)
	touch $(FLAGS)/.keep

$(FLAGS)/latest : $(FLAGS)/.keep
	curl -sS --fail "$(BASE)/latest" >$@~
	mv $@~ $@

$(FLAGS)/pgdata.index.gz : $(FLAGS)/latest
	curl -sS --fail "$(BASE)/$(LATEST)/pgdata.index.gz" >$@~
	mv $@~ $@

$(FLAGS)/pgdata.index : $(FLAGS)/pgdata.index.gz
	zcat $^ >$@~
	mv $@~ $@

$(FLAGS)/fetch.mk : $(FLAGS)/pgdata.index
	sed 's,^\([^\t]*\)\t.*,fetch : $(FLAGS)/\1.done,' $^ >$@~
	mv $@~ $@

-include $(FLAGS)/fetch.mk

$(FLAGS)/df-ok : $(FLAGS)/pgdata.index
	awk -v "AVAIL=`stat --file-system --format='%a * %S' "$(DEST)" | bc`" '{ need += $$2 } END { if (need > AVAIL) { printf("Only %.1f GiB available, need %.1f GiB\n", AVAIL / 1073741824, need / 1073741824); exit(1) } }' $^
	touch $@

$(FLAGS)/%.done : $(FLAGS)/df-ok
	curl -sS --fail "$(BASE)/$(LATEST)/$*" | tar --extract --xz --file - --directory "$(DEST)"
	touch $@

fetch : $(FLAGS)/recovery.conf

# It's okay to have that simple `restore_command` as PostgreSQL explicitly sets
# %p to temporary file and `xzcat` signals on failures.
$(FLAGS)/recovery.conf : $(FLAGS)/df-ok
	echo "standby_mode = 'on'" >$@~
	echo "restore_command = 'curl -x <%= @http_proxy %> --fail -sS $(WAL)/%f.xz | xzcat >%p'" >>$@~
	cp $@~ $(DEST)/recovery.conf
	mv $@~ $@
